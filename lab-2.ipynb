{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from typing import Tuple\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import json\n",
    "import sklearn\n",
    "\n",
    "dataset = pd.read_csv('data/multisession-eeg.csv')\n",
    "fromstring = lambda array_str: np.fromstring(array_str, dtype=float, sep=',')\n",
    "dataset.raw_fft = dataset.raw_fft.apply(fromstring)\n",
    "# dataset.raw_fft.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passthoughts\n",
    "\n",
    "What if you could simply *think your password*? That's the premise behind *passthoughts*. We'll discuss passthoughts in more depth in lecture 3, but for now, we'll lay this out as a classification problem:\n",
    "\n",
    "> Given a reading, and a person, is that person who they claim to be?\n",
    "\n",
    "We'll structure this problem as follows: For each subject, we'll train a classifier. That subject's readings will be positive example, and everyone else's readings will be negative examples.\n",
    "\n",
    "We can make this a little fancier by having people use specific thoughts (e.g. \"focus on your breathing,\" \"sing a song in your head,\" etc). We'll make sure our methods can handle this case, but for the time being, we'll just use the `\"unabeled\"` readings - people doing nothing in particular.\n",
    "\n",
    "We'll use subject `A` as our \"target\" individual. We will train on this subject for this assignment, and train against the other subjects in the corpus (subjects `B` and `C`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_matrix (series):\n",
    "    return np.array([ x for x in series ])\n",
    "\n",
    "def readings_right_subject_right_task (subj, task, session=0):\n",
    "    return to_matrix(dataset[\n",
    "        (dataset['subject'] == subj) &\n",
    "        (dataset['label'] == task) &\n",
    "        (dataset['session'] == session)\n",
    "    ].raw_fft)\n",
    "\n",
    "def readings_wrong_subj_any_task (subj):\n",
    "    return to_matrix(dataset[\n",
    "        (dataset['subject'] != subj)\n",
    "    ].raw_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['unlabeled', 'breathe', 'song', 'song_o', 'sport', 'breathe_o',\n",
       "       'speech', 'face', 'calibration', 'word_x', 'phrase_x', 'face_x',\n",
       "       'breatheopen', 'song_x', 'sport_x', 'breatheclosed', 'word_c',\n",
       "       'phrase_c', 'face_c', 'song_c', 'sport_c'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 516), (40, 516))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive = readings_right_subject_right_task('A', 'unlabeled', 0)\n",
    "negative = readings_wrong_subj_any_task('A')\n",
    "positive.shape, positive.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "Notice how we structured our positive and negative examples:\n",
    "\n",
    "- *Positive examples*: The right person thinking the right task.\n",
    "\n",
    "- *Negative examples*: The wrong person thinking any task (whether it is right or wrong).\n",
    "\n",
    "In the context of passthoughts, consider other possibilites for selecting positive and negative features. Here, (1) pick one configuration of positive and negative examples, aside from the ones listed, and (2) discuss their possible consequences (pros/cons). Explain how you might evaluate this selection (with data, with user experiments, etc - your choice)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll turn these data into our feature/label matrices `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.concatenate([positive, negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array([ 0 for x in positive] + [ 1 for x in negative])\n",
    "assert X.shape[0] == y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are assigning `0` to \"positive\" examples, and `1` to \"negative\" examples. That means `0` will mean \"ACCEPT\" and `1` will mean \"REJECT.\"\n",
    "\n",
    "## TODO\n",
    "\n",
    "Now, train and test a classifier! Estimate your classifier's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here....\n",
    "\n",
    "def fresh_clf () -> XGBClassifier:\n",
    "    return XGBClassifier(\n",
    "        # Don't worry about those parameters for now,\n",
    "        # though feel free to look them up if you're interested.\n",
    "        objective= 'binary:logistic',\n",
    "        seed=27)\n",
    "\n",
    "def xgb_cross_validate (\n",
    "    X: np.array,\n",
    "    y: np.array,\n",
    "    nfold: int=7\n",
    ") -> Tuple[XGBClassifier, pd.DataFrame]:\n",
    "    # eval_metrics:\n",
    "    # http://xgboost.readthedocs.io/en/latest//parameter.html\n",
    "    metrics = ['error@0.1', 'auc']\n",
    "#     metrics = [ 'auc' ]\n",
    "    # we use the @ syntax to override the default of 0.5 as the threshold for 0 / 1 classification\n",
    "    # the intent here to to minimize FAR at the expense of FRR\n",
    "    alg = fresh_clf()\n",
    "    xgtrain = xgb.DMatrix(X,y)\n",
    "    param = alg.get_xgb_params()\n",
    "    cvresults = xgb.cv(param,\n",
    "                      xgtrain,\n",
    "                      num_boost_round=alg.get_params()['n_estimators'],\n",
    "                      nfold=nfold,\n",
    "                      metrics=metrics,\n",
    "                      early_stopping_rounds=100\n",
    "                      )\n",
    "    alg.set_params(n_estimators=cvresults.shape[0])\n",
    "    alg.fit(X,y,eval_metric=metrics)\n",
    "    return alg, cvresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = sklearn.model_selection.train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.33, \n",
    "    random_state=42)\n",
    "\n",
    "clf, cvres = xgb_cross_validate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98329355608591884"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For authentication, what we want even more than \"accuracy\" here are two metrics:\n",
    "\n",
    "- False Acceptance Rate (FAR): The percentage of readings *not* from subject A incorrectly classified \"ACCEPT.\"\n",
    "- False Rejection Rate (FRR): The percentage of readings *from* subject A incorrectly classified 'REJECT.\"\n",
    "\n",
    "For authentication /security/, we want FAR to be as low as possible (so nobody can break in).\n",
    "For authentication /usability/, we want FRR to be low (so user's don't get frustrated constantly re-trying their passthought)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def far_frr (classifier, features, labels):\n",
    "    # predict all the labels\n",
    "    y_pred = classifier.predict(features)\n",
    "    false_accepts = 0\n",
    "    false_rejects = 0\n",
    "    for predicted, actual in zip(y_pred, labels):\n",
    "        # if we should have rejected,\n",
    "        # but in fact accepted,\n",
    "        if (actual == 1) and (predicted == 0):\n",
    "            # increment false accepts\n",
    "            false_accepts += 1\n",
    "        # if we should have accepted,\n",
    "        # but in fact rejected,\n",
    "        if (actual == 0) and (predicted == 1):\n",
    "            # increment false rejections\n",
    "            false_rejects += 1\n",
    "    # calculate proportions for each\n",
    "    far = false_accepts / len(list(filter(lambda x: x==0, y_pred)))\n",
    "    frr = false_rejects / len(list(filter(lambda x: x==1, y_pred)))\n",
    "    return far, frr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FAR: 0.3 - FRR: 0.009779951100244499'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "far, frr = far_frr(clf, X_validate, y_validate)\n",
    "f'FAR: {far} - FRR: {frr}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, these results might be good. \n",
    "\n",
    "But our classifier's accuracy could be misleading.   \n",
    "\n",
    "Can you see why? \n",
    "\n",
    "# Nonstationarity\n",
    "\n",
    "We are training, and testing, using data recorded over a single session. As we know, EEG changes over time, a property known as *nonstationarity*. Will our great results still hold a few weeks later?\n",
    "\n",
    "Let's take subject `A`'s data from sessions 1 and 2, which were recorded a few weeks after session 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_subja_sess1 = readings_right_subject_right_task('B', 'unlabeled', 1)\n",
    "X_subja_sess2 = readings_right_subject_right_task('B', 'unlabeled', 2)\n",
    "X_subja_later = np.concatenate([X_subja_sess1, X_subja_sess2])\n",
    "y_subja_later = [ 1 for x in X_subja_later ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try the classifier we trained on the original data, testing it on the later data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input numpy.ndarray must be 2 dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e58223b1e890>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfar_frr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_subja_later\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_subja_later\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34mf'FAR: {far}% - FRR: {frr}%'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-461a714a5bb3>\u001b[0m in \u001b[0;36mfar_frr\u001b[0;34m(classifier, features, labels)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfar_frr\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# predict all the labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mfalse_accepts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfalse_rejects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/xgboost-0.6-py3.6.egg/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0mtest_dmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m         class_probs = self.booster().predict(test_dmatrix,\n\u001b[1;32m    483\u001b[0m                                              \u001b[0moutput_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_margin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/xgboost-0.6-py3.6.egg/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_npy2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/xgboost-0.6-py3.6.egg/xgboost/core.py\u001b[0m in \u001b[0;36m_init_from_npy2d\u001b[0;34m(self, mat, missing)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \"\"\"\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input numpy.ndarray must be 2 dimensional'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;31m# flatten the array by rows and ensure it is float32.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;31m# we try to avoid data copies if possible (reshape returns a view when possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input numpy.ndarray must be 2 dimensional"
     ]
    }
   ],
   "source": [
    "far, frr = far_frr(clf, X_subja_later, y_subja_later)\n",
    "f'FAR: {far}% - FRR: {frr}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will discuss more in lecture 3, this is a problem for us. After all, we can calibrate our target subject, but we then expect them to leave the lab and go use the device later on. If their state changes so much that they can no longer be authenticated, we can't very well claim our system is accurate!\n",
    "\n",
    "## TODO\n",
    "\n",
    "The crux of the lab focuses on nonstationarity. At minimum, your mission is to quantify and qualify *what* is changing in EEG signals over time. You may use any tools in answering this question.\n",
    "\n",
    "You also have your choice of corpus:\n",
    "\n",
    "- Study subject `A`'s recordings over the three sessions provided here.\n",
    "- Study one subject's recordings over the course of a year.\n",
    "\n",
    "You can use both of these corpora, if you would like.\n",
    "\n",
    "Some questions to spur investigation:\n",
    "\n",
    "- What features of readings cause a classifier that works on earlier recordings fail on later ones?\n",
    "- What features remain the same? Are there any?\n",
    "- What might be the source of these changing features? Changing placement in the EEG device? Changing properties of the brain?\n",
    "\n",
    "Please note below all work you do, and any notes you make along the way. Ideally, your work should read like a story - words (and questions!) interspersed with code. Good luck, and have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
