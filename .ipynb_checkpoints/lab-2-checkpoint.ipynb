{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riversj\\Anaconda3\\envs\\data-x\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:161: UserWarning: pylab import has clobbered these variables: ['fromstring']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from typing import Tuple\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import json\n",
    "import sklearn\n",
    "\n",
    "dataset = pd.read_csv('data/multisession-eeg.csv')\n",
    "fromstring = lambda array_str: np.fromstring(array_str, dtype=float, sep=',')\n",
    "dataset.raw_fft = dataset.raw_fft.apply(fromstring)\n",
    "# dataset.raw_fft.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passthoughts\n",
    "\n",
    "What if you could simply *think your password*? That's the premise behind *passthoughts*. We'll discuss passthoughts in more depth in lecture 3, but for now, we'll lay this out as a classification problem:\n",
    "\n",
    "> Given a reading, and a person, is that person who they claim to be?\n",
    "\n",
    "We'll structure this problem as follows: For each subject, we'll train a classifier. That subject's readings will be positive example, and everyone else's readings will be negative examples.\n",
    "\n",
    "We can make this a little fancier by having people use specific thoughts (e.g. \"focus on your breathing,\" \"sing a song in your head,\" etc). We'll make sure our methods can handle this case, but for the time being, we'll just use the `\"unabeled\"` readings - people doing nothing in particular.\n",
    "\n",
    "We'll use subject `A` as our \"target\" individual. We will train on this subject for this assignment, and train against the other subjects in the corpus (subjects `B` and `C`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_matrix (series):\n",
    "    return np.array([ x for x in series ])\n",
    "\n",
    "def readings_right_subject_right_task (subj, task, session=0):\n",
    "    return to_matrix(dataset[\n",
    "        (dataset['subject'] == subj) &\n",
    "        (dataset['label'] == task) &\n",
    "        (dataset['session'] == session)\n",
    "    ].raw_fft)\n",
    "\n",
    "def readings_wrong_subj_any_task (subj):\n",
    "    return to_matrix(dataset[\n",
    "        (dataset['subject'] != subj)\n",
    "    ].raw_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['unlabeled', 'breathe', 'song', 'song_o', 'sport', 'breatheopen',\n",
       "       'speech', 'face'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['label'].replace(to_replace='breathe_o', value='breatheopen', inplace=True)\n",
    "dataset[(dataset['subject']=='A') & (dataset['session'] == 0)]['label'].unique()\n",
    "#dataset['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 516), (1228, 516))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive = readings_right_subject_right_task('A', 'unlabeled', 0)\n",
    "negative = readings_wrong_subj_any_task('A')\n",
    "positive.shape, negative.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "Notice how we structured our positive and negative examples:\n",
    "\n",
    "- *Positive examples*: The right person thinking the right task.\n",
    "\n",
    "- *Negative examples*: The wrong person thinking any task (whether it is right or wrong).\n",
    "\n",
    "In the context of passthoughts, consider other possibilites for selecting positive and negative features. Here, (1) pick one configuration of positive and negative examples, aside from the ones listed, and (2) discuss their possible consequences (pros/cons). Explain how you might evaluate this selection (with data, with user experiments, etc - your choice)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Positive examples: The right person thinking the right task. The wrong person thinking the right task.\n",
    "- Negative examples: The right person thinking the wrong task.\n",
    "\n",
    "In the case of a normal password, although it is usually used by one person, the advantage of having a password that is written is that another person can be given the password if they need to use it. If we only look for the right person thinking the right task, we have a two-factor identification system, but it will make it hard or impossible for any other person to be given access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((92, 516), (199, 516))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def readings_right_subject_wrong_task (subj, task, session=0):\n",
    "    return to_matrix(dataset[\n",
    "        (dataset['subject'] == subj) &\n",
    "        (dataset['label'] != task) &\n",
    "        (dataset['session'] == session)\n",
    "    ].raw_fft)\n",
    "\n",
    "positive = np.concatenate((readings_right_subject_right_task('A', 'breatheopen', 0),\n",
    "                           readings_right_subject_right_task('B', 'breatheopen', 0),\n",
    "                           readings_right_subject_right_task('C', 'breatheopen', 0)))\n",
    "negative = readings_right_subject_wrong_task('A', 'breatheopen', 0)\n",
    "positive.shape, negative.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll turn these data into our feature/label matrices `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.concatenate([positive, negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array([ 0 for x in positive] + [ 1 for x in negative])\n",
    "assert X.shape[0] == y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are assigning `0` to \"positive\" examples, and `1` to \"negative\" examples. That means `0` will mean \"ACCEPT\" and `1` will mean \"REJECT.\"\n",
    "\n",
    "## TODO\n",
    "\n",
    "Now, train and test a classifier! Estimate your classifier's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here....\n",
    "\n",
    "def fresh_clf () -> XGBClassifier:\n",
    "    return XGBClassifier(\n",
    "        # Don't worry about those parameters for now,\n",
    "        # though feel free to look them up if you're interested.\n",
    "        objective= 'binary:logistic',\n",
    "        seed=27)\n",
    "\n",
    "def xgb_cross_validate (\n",
    "    X: np.array,\n",
    "    y: np.array,\n",
    "    nfold: int=7\n",
    ") -> Tuple[XGBClassifier, pd.DataFrame]:\n",
    "    # eval_metrics:\n",
    "    # http://xgboost.readthedocs.io/en/latest//parameter.html\n",
    "    metrics = ['error@0.1', 'auc']\n",
    "#     metrics = [ 'auc' ]\n",
    "    # we use the @ syntax to override the default of 0.5 as the threshold for 0 / 1 classification\n",
    "    # the intent here to to minimize FAR at the expense of FRR\n",
    "    alg = fresh_clf()\n",
    "    xgtrain = xgb.DMatrix(X,y)\n",
    "    param = alg.get_xgb_params()\n",
    "    cvresults = xgb.cv(param,\n",
    "                      xgtrain,\n",
    "                      num_boost_round=alg.get_params()['n_estimators'],\n",
    "                      nfold=nfold,\n",
    "                      metrics=metrics,\n",
    "                      early_stopping_rounds=100\n",
    "                      )\n",
    "    alg.set_params(n_estimators=cvresults.shape[0])\n",
    "    alg.fit(X,y,eval_metric=metrics)\n",
    "    return alg, cvresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = sklearn.model_selection.train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.33, \n",
    "    random_state=42)\n",
    "\n",
    "clf, cvres = xgb_cross_validate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89690721649484539"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For authentication, what we want even more than \"accuracy\" here are two metrics:\n",
    "\n",
    "- False Acceptance Rate (FAR): The percentage of readings *not* from subject A incorrectly classified \"ACCEPT.\"\n",
    "- False Rejection Rate (FRR): The percentage of readings *from* subject A incorrectly classified 'REJECT.\"\n",
    "\n",
    "For authentication /security/, we want FAR to be as low as possible (so nobody can break in).\n",
    "For authentication /usability/, we want FRR to be low (so user's don't get frustrated constantly re-trying their passthought)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def far_frr (classifier, features, labels):\n",
    "    # predict all the labels\n",
    "    y_pred = classifier.predict(features)\n",
    "    false_accepts = 0\n",
    "    false_rejects = 0\n",
    "    for predicted, actual in zip(y_pred, labels):\n",
    "        # if we should have rejected,\n",
    "        # but in fact accepted,\n",
    "        if (actual == 1) and (predicted == 0):\n",
    "            # increment false accepts\n",
    "            false_accepts += 1\n",
    "        # if we should have accepted,\n",
    "        # but in fact rejected,\n",
    "        if (actual == 0) and (predicted == 1):\n",
    "            # increment false rejections\n",
    "            false_rejects += 1\n",
    "    # calculate proportions for each\n",
    "    num_intruders = len(list(filter(lambda x: x==1, labels)))\n",
    "    far = false_accepts / num_intruders if num_intruders > 0 else 0\n",
    "    num_valids = len(list(filter(lambda x: x==0, labels)))\n",
    "    frr = false_rejects / num_valids if num_valids > 0 else 0\n",
    "    return far, frr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FAR: 0.0% - FRR: 28.57142857142857%'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "far, frr = far_frr(clf, X_validate, y_validate)\n",
    "f'FAR: {far*100}% - FRR: {frr*100}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, these results might be good. \n",
    "\n",
    "But our classifier's accuracy could be misleading.   \n",
    "\n",
    "Can you see why? \n",
    "\n",
    "# Nonstationarity\n",
    "\n",
    "We are training, and testing, using data recorded over a single session. As we know, EEG changes over time, a property known as *nonstationarity*. Will our great results still hold a few weeks later?\n",
    "\n",
    "Let's take subject `A`'s data from sessions 1 and 2, which were recorded a few weeks after session 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(616, 516)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_subja_sess1 = readings_right_subject_right_task('A', 'unlabeled', 1)\n",
    "X_subja_sess2 = readings_right_subject_right_task('A', 'unlabeled', 2)\n",
    "X_subja_later = np.concatenate([X_subja_sess1, X_subja_sess2])\n",
    "y_subja_later = [ 0 for x in X_subja_later ]\n",
    "X_subja_later.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try the classifier we trained on the original data, testing it on the later data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FAR: 0% - FRR: 39.935064935064936%'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "far, frr = far_frr(clf, X_subja_later, y_subja_later)\n",
    "f'FAR: {far*100}% - FRR: {frr*100}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will discuss more in lecture 3, this is a problem for us. After all, we can calibrate our target subject, but we then expect them to leave the lab and go use the device later on. If their state changes so much that they can no longer be authenticated, we can't very well claim our system is accurate!\n",
    "\n",
    "## TODO\n",
    "\n",
    "The crux of the lab focuses on nonstationarity. At minimum, your mission is to quantify and qualify *what* is changing in EEG signals over time. You may use any tools in answering this question.\n",
    "\n",
    "You also have your choice of corpus:\n",
    "\n",
    "- Study subject `A`'s recordings over the three sessions provided here.\n",
    "- Study one subject's recordings over the course of a year.\n",
    "\n",
    "You can use both of these corpora, if you would like.\n",
    "\n",
    "Some questions to spur investigation:\n",
    "\n",
    "- What features of readings cause a classifier that works on earlier recordings fail on later ones?\n",
    "- What features remain the same? Are there any?\n",
    "- What might be the source of these changing features? Changing placement in the EEG device? Changing properties of the brain?\n",
    "\n",
    "Please note below all work you do, and any notes you make along the way. Ideally, your work should read like a story - words (and questions!) interspersed with code. Good luck, and have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Nonstationarity Exploration\n",
    "\n",
    "First we need to look at which tasks/passthoughts are actually used in all 3 sessions with A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 0: ['unlabeled' 'breathe' 'song' 'song_o' 'sport' 'breatheopen' 'speech'\n",
      " 'face']\n",
      "Session 1: ['unlabeled' 'calibration' 'word_x' 'phrase_x' 'face_x' 'breatheopen'\n",
      " 'song_x' 'sport_x']\n",
      "Session 2: ['unlabeled' 'calibration' 'breatheclosed' 'word_x' 'word_c' 'phrase_x'\n",
      " 'phrase_c' 'face_x' 'face_c' 'breatheopen' 'song_x' 'song_c' 'sport_x'\n",
      " 'sport_c']\n"
     ]
    }
   ],
   "source": [
    "print('Session 0:', dataset[(dataset['subject']=='A') & (dataset['session'] == 0)]['label'].unique())\n",
    "print('Session 1:', dataset[(dataset['subject']=='A') & (dataset['session'] == 1)]['label'].unique())\n",
    "print('Session 2:', dataset[(dataset['subject']=='A') & (dataset['session'] == 2)]['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming \"song\" lines up with \"song_x\" and similarly for \"sport\" and \"face\", there are four tasks repeated in all three sessions. For some reason, it seems that the tasks lasted longer in the last session. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 1: (24, 5)\n",
      "Session 2: (46, 5)\n"
     ]
    }
   ],
   "source": [
    "print('Session 1:', dataset[(dataset['subject']=='A') & (dataset['session'] == 1) & (dataset['label'] == 'face_x')].shape)\n",
    "print('Session 2:', dataset[(dataset['subject']=='A') & (dataset['session'] == 2) & (dataset['label'] == 'face_x')].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note this, but ignore it from here on out.\n",
    "\n",
    "We now isolate these 4 variables from the dataset. This is in order to avoid having large amounts of data from one session over the others (although as noted above we will still have larger amounts from session 2 than 1 or 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(386, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['label'].replace(['song', 'sport', 'face'], ['song_x', 'sport_x', 'face_x'], inplace = True)\n",
    "A_data = dataset[(dataset['label'].isin(['song_x', 'sport_x', 'face_x', 'breatheopen'])) & (dataset['subject'] == 'A')]\n",
    "A_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train our classifier and see how it does if trained across sessions. We can also test it on session 2 after training it on 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def readings_right_subject_right_task (dataset, subj, task, session=0):\n",
    "    return to_matrix(dataset[\n",
    "        (dataset['subject'] == subj) &\n",
    "        (dataset['label'] == task) &\n",
    "        (dataset['session'] == session)\n",
    "    ].raw_fft)\n",
    "\n",
    "def readings_right_subject_wrong_task (dataset, subj, task, session=0):\n",
    "    return to_matrix(dataset[\n",
    "        (dataset['subject'] == subj) &\n",
    "        (dataset['label'] != task) &\n",
    "        (dataset['session'] == session)\n",
    "    ].raw_fft)\n",
    "\n",
    "positive = np.concatenate((readings_right_subject_right_task(A_data, 'A', 'song_x', 0),\n",
    "                            readings_right_subject_right_task(A_data, 'A', 'song_x', 1),\n",
    "                            readings_right_subject_right_task(A_data, 'A', 'song_x', 2)))\n",
    "negative = np.concatenate((readings_right_subject_wrong_task(A_data, 'A', 'song_x', 0),\n",
    "                           readings_right_subject_wrong_task(A_data, 'A', 'song_x', 1),\n",
    "                           readings_right_subject_wrong_task(A_data, 'A', 'song_x', 2)))\n",
    "positive.shape[0] + negative.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.concatenate([positive, negative])\n",
    "y = np.array([ 0 for x in positive] + [ 1 for x in negative])\n",
    "assert X.shape[0] == y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = sklearn.model_selection.train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.33, \n",
    "    random_state=42)\n",
    "\n",
    "clf, cvres = xgb_cross_validate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7109375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'FAR: 5.555555555555555% - FRR: 84.21052631578947%'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(clf.score(X_validate, y_validate))\n",
    "far, frr = far_frr(clf, X_validate, y_validate)\n",
    "f'FAR: {far*100}% - FRR: {frr*100}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much worse than I was expecting. It seems like there is very little in common from one session to the next. A somewhat high FRR would not surprise me greatly since there were many more false attempts than true attempts, but an FAR of 5% is much higher than I would expect (especially given how high the FRR is). Clearly, the classifier is not just outright rejecting all attempts, yet it still is managing to reject more than half of the valid attempts. \n",
    "\n",
    "Even the overall score (71%) is much lower than we have seen with single session classification. We can try with other labels and see if they perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sport_x 0.703125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'FAR: 1.1235955056179776% - FRR: 94.87179487179486%'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Create_pos_neg(A_data, label):\n",
    "    positive = np.concatenate((readings_right_subject_right_task(A_data, 'A', label, 0),\n",
    "                                readings_right_subject_right_task(A_data, 'A', label, 1),\n",
    "                                readings_right_subject_right_task(A_data, 'A', label, 2)))\n",
    "    negative = np.concatenate((readings_right_subject_wrong_task(A_data, 'A', label, 0),\n",
    "                               readings_right_subject_wrong_task(A_data, 'A', label, 1),\n",
    "                               readings_right_subject_wrong_task(A_data, 'A', label, 2)))\n",
    "    return (positive, negative)\n",
    "\n",
    "positive, negative = Create_pos_neg(A_data, 'sport_x')\n",
    "X = np.concatenate([positive, negative])\n",
    "y = np.array([ 0 for x in positive] + [ 1 for x in negative])\n",
    "assert X.shape[0] == y.shape[0]\n",
    "X_train, X_validate, y_train, y_validate = sklearn.model_selection.train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.33, \n",
    "    random_state=42)\n",
    "\n",
    "clf, cvres = xgb_cross_validate(X_train, y_train)\n",
    "print('For sport_x', clf.score(X_validate, y_validate))\n",
    "far, frr = far_frr(clf, X_validate, y_validate)\n",
    "f'FAR: {far*100}% - FRR: {frr*100}%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For face_x 0.8046875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'FAR: 4.3478260869565215% - FRR: 58.333333333333336%'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive, negative = Create_pos_neg(A_data, 'face_x')\n",
    "X = np.concatenate([positive, negative])\n",
    "y = np.array([ 0 for x in positive] + [ 1 for x in negative])\n",
    "assert X.shape[0] == y.shape[0]\n",
    "X_train, X_validate, y_train, y_validate = sklearn.model_selection.train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.33, \n",
    "    random_state=42)\n",
    "\n",
    "clf, cvres = xgb_cross_validate(X_train, y_train)\n",
    "print('For face_x', clf.score(X_validate, y_validate))\n",
    "far, frr = far_frr(clf, X_validate, y_validate)\n",
    "f'FAR: {far*100}% - FRR: {frr*100}%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For breatheopen 0.7265625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'FAR: 2.1739130434782608% - FRR: 91.66666666666666%'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive, negative = Create_pos_neg(A_data, 'breatheopen')\n",
    "X = np.concatenate([positive, negative])\n",
    "y = np.array([ 0 for x in positive] + [ 1 for x in negative])\n",
    "assert X.shape[0] == y.shape[0]\n",
    "X_train, X_validate, y_train, y_validate = sklearn.model_selection.train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.33, \n",
    "    random_state=42)\n",
    "\n",
    "clf, cvres = xgb_cross_validate(X_train, y_train)\n",
    "print('For breatheopen', clf.score(X_validate, y_validate))\n",
    "far, frr = far_frr(clf, X_validate, y_validate)\n",
    "f'FAR: {far*100}% - FRR: {frr*100}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier seems to perform poorly for all of these labels. This seems to indicate that the nonstationarity, at least in this data sample, is so bad that the same classifier will not work across multiple sessions. \n",
    "\n",
    "One thing to keep in mind is that all imposter attempts in this scenario were from subject A, so there is a possibility that training the classifier against other subjects would improve the scores. We essentially only had one factor of identification for this scenario which was the knowledge factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fresh_clf () -> XGBClassifier:\n",
    "    return XGBClassifier(\n",
    "        # Don't worry about those parameters for now,\n",
    "        # though feel free to look them up if you're interested.\n",
    "        objective= 'binary:logistic',\n",
    "        seed=27)\n",
    "\n",
    "def xgb_cross_validate (\n",
    "    X: np.array,\n",
    "    y: np.array,\n",
    "    nfold: int=7\n",
    ") -> Tuple[XGBClassifier, pd.DataFrame]:\n",
    "    # eval_metrics:\n",
    "    # http://xgboost.readthedocs.io/en/latest//parameter.html\n",
    "    metrics = ['error@0.9']\n",
    "#     metrics = [ 'auc' ]\n",
    "    # we use the @ syntax to override the default of 0.5 as the threshold for 0 / 1 classification\n",
    "    # the intent here to to minimize FAR at the expense of FRR\n",
    "    alg = fresh_clf()\n",
    "    xgtrain = xgb.DMatrix(X,y)\n",
    "    param = alg.get_xgb_params()\n",
    "    cvresults = xgb.cv(param,\n",
    "                      xgtrain,\n",
    "                      num_boost_round=alg.get_params()['n_estimators'],\n",
    "                      nfold=nfold,\n",
    "                      metrics=metrics,\n",
    "                      early_stopping_rounds=100\n",
    "                      )\n",
    "    alg.set_params(n_estimators=cvresults.shape[0])\n",
    "    alg.fit(X,y,eval_metric=metrics)\n",
    "    return alg, cvresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For face_x 0.8046875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'FAR: 4.3478260869565215% - FRR: 58.333333333333336%'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive, negative = Create_pos_neg(A_data, 'face_x')\n",
    "X = np.concatenate([positive, negative])\n",
    "y = np.array([ 0 for x in positive] + [ 1 for x in negative])\n",
    "assert X.shape[0] == y.shape[0]\n",
    "X_train, X_validate, y_train, y_validate = sklearn.model_selection.train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.33, \n",
    "    random_state=42)\n",
    "\n",
    "clf, cvres = xgb_cross_validate(X_train, y_train)\n",
    "print('For face_x', clf.score(X_validate, y_validate))\n",
    "far, frr = far_frr(clf, X_validate, y_validate)\n",
    "f'FAR: {far*100}% - FRR: {frr*100}%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
